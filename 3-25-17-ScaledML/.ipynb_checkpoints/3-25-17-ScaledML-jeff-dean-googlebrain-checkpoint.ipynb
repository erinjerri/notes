{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google brain team : research impact \n",
    "\n",
    "* Since 2012, published > 130 \n",
    "\n",
    "### Main Research Areas\n",
    " * Gen ML, Algos and Techniques \n",
    " * C Sys for ML\n",
    " * NL Understanding\n",
    " * perception\n",
    " * Healthcare\n",
    " * Robotics\n",
    " * Music and Art Generation\n",
    "    [Link] (research.googleblog.com/2017/01/the-google-brain-team-looking-back-on.html)\n",
    "    \n",
    "### Experiment = hella days\n",
    "* Turnaround Time and Research Productiviyt\n",
    "*Minutes, Hours - Interactive research, Instat gratification!\n",
    "* 1-3 days - tolerable. \n",
    "* 1-4 weeks high value experiemnts only. \n",
    "* Progress stalls.\n",
    "> 1 month - don't even try\n",
    "\n",
    "## TF Overview: Computation is a dataflow graph - with state\n",
    "// Jeff's comments on infographic\n",
    "* state you maintain, update at end of graph\n",
    "* computational garphs - abstract - map onto 1 or mutliple computing devices\n",
    "* Terminology: 3-4D arrays - tensors\n",
    "\n",
    "### Same mechanisim supports large distributed systems\n",
    "* Computation spread across hundreds of distributed systems \n",
    "\n",
    "### What Did We Build TensorFlow?\n",
    "* Wanted system that was flexible, scalba,e and production-ready\n",
    "* DistBelief, our first system, was good on two of these, but lacked flexibilty\n",
    "* Most existing open source packages were good on 2 of 3, but not all 3\n",
    "\n",
    "### Just-In-Time Compilation via XLA, Accerlated Lineary Algebra compilerTf graphi go in\n",
    "* Optimized and specialized assembly comes out. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo: Inspirct JTI Code in TF iPython shell\n",
    "    * Qualcomm snapdragon for embedded devices\n",
    "    \n",
    "    Computers can now see large implications for healthcare\n",
    "    \n",
    "    // Lily's group\n",
    "    Medical imaging\n",
    "    * Using similar model for detecting diabetic retinopahty in retinal images\n",
    "    arxiv.org/abs/1703.02442\n",
    "    [Assisting Pathologists in Detecting Cancer with Deep Learning](https://research.googleblog.com/2017/03/assisting-pathologists-in-detecting.html) \n",
    "    \n",
    "    Pathology - localized pathology images (cancer)\n",
    "    Extremely large images ( > 100kx100k pixels)\n",
    "    Multiscale problem - need detail a well as context\n",
    "    \n",
    "## Multiscale model\n",
    "* detail <--> context\n",
    "* resembles micrscope magnifications\n",
    "* Jeff's comments: using inception v3, pretrained on imagenet\n",
    "    prediction on cancer\n",
    "    *   detecting breast cancr metastass in lymp nodes\n",
    "    \n",
    "    biopsy image, ground truth (from pathologist) model precdiction,early results\n",
    "    \n",
    "    model pre\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other NLP \n",
    "* Sequence-to-sequence model: machine translation\n",
    "* Sutskever and Vinyals & Le  NIPS 2014 \n",
    "* At inference time, beam search to chose most probaly over possible output seq\n",
    "// Sequence to Sequence model applied to Google Translate\n",
    "\n",
    "Infog: Google Neural Machine Translation Model\n",
    "\n",
    "[Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](http://www.arxiv.org/abs/1609.08144)\n",
    "* One model replica: one machine w/ 8 GPUs\n",
    "* Encoder LSTMs -> attnt \n",
    "* Decoder LSTMs -> Softmax\n",
    "\n",
    "[Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation](http://research.google.blog.com/2016/09/a-neural-network-for-machine.html)\n",
    "\n",
    "[Zero-Shot Translation with Googleâ€™s Multilingual Neural Machine Translation System]\n",
    "(https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html)\n",
    "\n",
    "Bigger models, but sparsely activated \n",
    "Motivation: want huge model capacity\n",
    "\n",
    "Infographic: \n",
    "* Per-Ex Routing\n",
    "* Feed data into gating network\n",
    "* MoE layers \n",
    "* Sparse, routed to a number of experts\n",
    "\n",
    "[Outrageously Large Neural Networks: The Sparsely-gated Mixture-of-Experts Layer](https://openreview.net/pdf?id=B1ckMDqlg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-2407b49a1992>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-2407b49a1992>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Current: Solution = ML expertise + data + computation\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Current: Solution = ML expertise + data + computation\n",
    "    Can we turn this to \n",
    "    Solution - data + 100x computation (w/o ML expertise) to solution?\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early encouraging signs\n",
    "\n",
    "### Trying multple diff approaches:\n",
    "    1. RL-based architecture search\n",
    "    2. Model search\n",
    "    \n",
    "    To appear in ICLR 2017\n",
    "    \n",
    "    Barret Zph, Quoc V. Le\n",
    "    Google Brain\n",
    "    barretzoph, qvl@ google.com\n",
    "    Idea; model-generation model trained via RL\n",
    "    \n",
    "    1. Generate 10 models\n",
    "    2. Train them for a few hours\n",
    "    3. Use loss of the generated models as reinforcement learning signal\n",
    "    \n",
    "[Neural Architecture Search with Reinforcement Learning] (http://www.arxiv.org/abs/1611.01578)\n",
    "    \n",
    "* Normal LSTM Cell\n",
    "* Cell discovered by architecture search \n",
    "\n",
    "[Large-Scale Evolution of Image Classifiers] (http://arxiv.org/abs/1703.01041)\n",
    "* Infog: DNA --> Model --> Trained model -> Fitness\n",
    "\n",
    "* Evolve From Scratch\n",
    "* Initialize w/ Linear Models\n",
    "* Repeat Evolutionary Step\n",
    "\n",
    "### Where are we going?\n",
    "*  Combine Several of these Ideas\n",
    "* Large model but sparsely activated\n",
    "* Single model to solve many tasks (100s to 1Ms)\n",
    "* Dynamically learn and grow pathways through large model\n",
    "\n",
    "### Infographic \n",
    "* Outputs - single large model, sparsely activated\n",
    "* Tasks \n",
    "\n",
    "* More comptuational power needed\n",
    "* Deep learning is transforming how we design computers\n",
    "\n",
    "### Special precision\n",
    "* reduced precision ok\n",
    "\n",
    "* handful of specific operations \n",
    "* linear algebra - would reduce precision\n",
    "\n",
    "### TPU - Tensor Processing Unit\n",
    "* Custom Google-designed chip for neural net computations\n",
    "\n",
    "* In production use for > 24 months: used on every search query, for neural machine translation, for AlphaGo match,...\n",
    "* Talk on Computer History Museum on April 5 \n",
    "\n",
    "sites.google.com/view/naeregionalsymposium\n",
    "\n",
    "* For large models, model paralellism is imp\n",
    "* But getting good perf - not easy - given multiple computing devices is non-trivial and non-obvious\n",
    "\n",
    "### Infog: Architecture ex. \n",
    "* Distributed\n",
    "* Softmax Attention, two other GPUs - 4 GPU cards\n",
    "\n",
    "## Reinforcement Learning for High Perfomance Machine Learning Models\n",
    "\n",
    "* Placement model (trained via RL) gets graph as input _ set of devices, outputs device placement for each graph note\n",
    "* placement --> Environment --> RT --update placement \n",
    "* measured time per step gives RL reward signal\n",
    "\n",
    "### Early results\n",
    "\n",
    "// Model paralleism\n",
    "* Per-step running times (secs) \n",
    "* Baselines: NMT human expert placement shown on earlier slide\n",
    "* Inception: default placement on GPU/0\n",
    "\n",
    "## Conclusions: DNN\n",
    "\n",
    "[Site] (g.co/brain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
